{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as f\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General approach\n",
    "\n",
    "The goal is to find a maximum-weight bipartite matching where the cost function is unknown and depends on features of the inputs. Additionally, we might want a matching that trades off optimality and diversity by way of entropy regularization -- conveniently, this is what Sinkhorn does.\n",
    "\n",
    "We have elements from the two sides $i, j$ with features $u_i, v_j$, the true cost is some $C^*_{ij}$, and we want to learn our own $C_{ij} = f(u_i, v_j; \\theta)$. In one interesting case many of the $C^*_{ij}$ may be 0, meaning these edges do not exist and should not be matched.\n",
    "\n",
    "Two approaches can be taken:\n",
    "\n",
    "1. Two-stage approach\n",
    "\n",
    "The two-stage approach is simple. One simply learns $f$ to predict the true observed $C^*_{ij}$ as well as possible, and then make matches based on the predictions of $f$. This approach should work well if $f$ can successfully learn the correct matching based on the features. There are some experimental details here -- which information exactly does $f$ get to observe -- do we get to see pairs of unmatched nodes, or only the actual matches made? $f$ here can be any learner.\n",
    "\n",
    "2. Predict-and-optimize approach\n",
    "\n",
    "The idea here is to still have $f$ output cost functions, but then pass these into a differentiable surrogate for the matching (Sinkhorn), and maximize the total weight of the observed matching. We expect this approach to win if $f$ cannot make perfect predictions. $f$ here must be a gradient based approach, i.e. a (shallow) NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some notes\n",
    "\n",
    "One advantage of this compared to the Wilder et al paper is that for a problem this simple, Sinkhorn is much nicer to deal with than their Gurobi nightmare or even qpth, and can stay on the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## first steps\n",
    "\n",
    "Consider the case where we either have edges or don't, and want to predict that from features only, in order to choose a maximum-weight matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 10\n",
    "N = 100\n",
    "u_feats = torch.rand(N, dim)\n",
    "v_feats = torch.rand(N, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_mat = u_feats @ (v_feats.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_mat = (scores_mat > 2.5).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training, batching, etc.\n",
    "\n",
    "first just train on dataset to get architecture up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(*[nn.Linear(dim*2, 128), nn.ReLU(), nn.Linear(128, 128), nn.ReLU(), nn.Linear(128, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=20, out_features=128, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### future notes\n",
    "\n",
    "eventually we will need a case (for entropic matching) where many of the inputs are indistinguishable -- maybe just discrete and/or binary input features.\n",
    "\n",
    "should try hiding some of the true features\n",
    "\n",
    "eventually will need a more interesting function to predict edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
